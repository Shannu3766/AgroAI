{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:27:03.644878Z",
     "iopub.status.busy": "2025-06-05T08:27:03.644512Z",
     "iopub.status.idle": "2025-06-05T08:27:33.371508Z",
     "shell.execute_reply": "2025-06-05T08:27:33.370772Z",
     "shell.execute_reply.started": "2025-06-05T08:27:03.644844Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 08:27:19.507612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749112039.715972      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749112039.782291      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "# import unsloth # Remove\n",
    "import torch\n",
    "# from unsloth import FastLanguageModel # Remove\n",
    "import pickle # Not used in the provided snippet, can be removed if not used elsewhere\n",
    "import os # Not used in the provided snippet, can be removed if not used elsewhere\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer \n",
    "from peft import PeftModel # Add\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:28:27.221806Z",
     "iopub.status.busy": "2025-06-05T08:28:27.221423Z",
     "iopub.status.idle": "2025-06-05T08:28:27.226685Z",
     "shell.execute_reply": "2025-06-05T08:28:27.225390Z",
     "shell.execute_reply.started": "2025-06-05T08:28:27.221780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "adapter_repo_id = \"aryan6637/deepseek-crop-fertilizer-info-v3\"\n",
    "base_model_name = \"deepseek-ai/deepseek-llm-7b-base\" # Base model used for fine-tuning\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:28:27.622070Z",
     "iopub.status.busy": "2025-06-05T08:28:27.621749Z",
     "iopub.status.idle": "2025-06-05T08:30:35.540432Z",
     "shell.execute_reply": "2025-06-05T08:30:35.539072Z",
     "shell.execute_reply.started": "2025-06-05T08:28:27.622047Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model deepseek-ai/deepseek-llm-7b-base for CPU...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ff5c2675294c6f8eb700c33e74af53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/584 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ee8592d2b844d69f85ea351dc6cb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/22.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcad5cb615741a68a105efb34fa9919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96906b0faf384f0a80ee4b91138ed498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a58bb309b0484c8e3973c651f0794f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed12826bf4dd4e7f8b7b6d0aec770154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697725b0e14a498dae663f4ca3d86219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d11e142336c4ca1a9d5287277d3643a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from aryan6637/deepseek-crop-fertilizer-info-v3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aae2ddf8964445295c3c336f82779be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbf26aaaf09462587f6c1b67a4eadc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.50M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a744d855895649f28be2a7924a8bf682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/355 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapter from aryan6637/deepseek-crop-fertilizer-info-v3 and merging...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbe9349e8dd4ad5b5b59a67547e6e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4610caae2e40599c984604bde8cf2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/150M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully on CPU.\n",
      "WARNING: Running a 7B parameter model on CPU will be very slow for inference.\n",
      "Updating model.config.pad_token_id to 100015\n",
      "Pad token: <|PAD_TOKEN|>, ID: 100015\n",
      "EOS token: <｜end▁of▁sentence｜>, ID: 100001\n"
     ]
    }
   ],
   "source": [
    "# Cell 5\n",
    "print(f\"Loading base model {base_model_name} for CPU...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float32,  # Use float32 for CPU; bfloat16 might work on some newer CPUs\n",
    "    # device_map=\"auto\" will try to use GPU if available, then CPU.\n",
    "    # Forcing CPU:\n",
    "    # device_map=\"cpu\", # Or handle with .to('cpu') later\n",
    ")\n",
    "\n",
    "print(f\"Loading tokenizer from {adapter_repo_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_repo_id, trust_remote_code=True) # Some tokenizers need this\n",
    "\n",
    "print(f\"Loading adapter from {adapter_repo_id} and merging...\")\n",
    "# Load the LoRA adapter\n",
    "model = PeftModel.from_pretrained(model, adapter_repo_id)\n",
    "# Merge the adapter into the base model for potentially faster CPU inference\n",
    "# This increases memory usage as it creates a full model.\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.to(\"cpu\") # Explicitly move to CPU\n",
    "print(\"Model and tokenizer loaded successfully on CPU.\")\n",
    "print(\"WARNING: Running a 7B parameter model on CPU will be very slow for inference.\")\n",
    "\n",
    "\n",
    "# Handle tokenizer padding - Unsloth logs indicate <|PAD_TOKEN|> is used.\n",
    "if tokenizer.pad_token is None or tokenizer.pad_token_id is None:\n",
    "    print(\"Tokenizer does not have a pad token or pad_token_id. Adding '<|PAD_TOKEN|>'.\")\n",
    "    # Check if the token already exists to avoid adding duplicates if config is weird\n",
    "    if '<|PAD_TOKEN|>' not in tokenizer.get_vocab():\n",
    "        tokenizer.add_special_tokens({'pad_token': '<|PAD_TOKEN|>'})\n",
    "    else:\n",
    "        tokenizer.pad_token = '<|PAD_TOKEN|>'\n",
    "    # After adding or setting, ensure pad_token_id is correct\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "    # Resize model embeddings if new tokens were truly added (not just setting existing)\n",
    "    # This is crucial if add_special_tokens actually modified the vocab size\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"Resized model embeddings to: {len(tokenizer)}\")\n",
    "\n",
    "\n",
    "# Ensure the model's config pad_token_id is also updated to match the tokenizer\n",
    "if model.config.pad_token_id is None or model.config.pad_token_id != tokenizer.pad_token_id:\n",
    "    print(f\"Updating model.config.pad_token_id to {tokenizer.pad_token_id}\")\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Pad token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}, ID: {tokenizer.eos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:31:09.769973Z",
     "iopub.status.busy": "2025-06-05T08:31:09.769638Z",
     "iopub.status.idle": "2025-06-05T08:31:09.775233Z",
     "shell.execute_reply": "2025-06-05T08:31:09.774311Z",
     "shell.execute_reply.started": "2025-06-05T08:31:09.769951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_inference_prompt(final_parameters):\n",
    "    instruction_text = (\n",
    "        f\"Given the following soil and environmental parameters:\\n\"\n",
    "        f\"- Temperature: {final_parameters['Temparature']}°C\\n\"\n",
    "        f\"- Humidity: {final_parameters['Humidity']}%\\n\"\n",
    "        f\"- Moisture: {final_parameters['Moisture']}\\n\"\n",
    "        f\"- Soil Type: {final_parameters['Soil Type']}\\n\"\n",
    "        f\"- Nitrogen: {final_parameters['Nitrogen']} ppm\\n\"\n",
    "        f\"- Potassium: {final_parameters['Potassium']} ppm\\n\"\n",
    "        f\"- Phosphorous: {final_parameters['Phosphorous']} ppm\\n\\n\"\n",
    "        f\"Predict the suitable Crop Type and Fertilizer Name, and provide brief information about how they work or their characteristics.\"\n",
    "    )\n",
    "    # Alpaca format - the model was trained to generate text after \"### Response:\\n\"\n",
    "    formatted_prompt = f\"### Instruction:\\n{instruction_text}\\n\\n### Response:\\n\"\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:31:10.531848Z",
     "iopub.status.busy": "2025-06-05T08:31:10.531012Z",
     "iopub.status.idle": "2025-06-05T08:31:10.538853Z",
     "shell.execute_reply": "2025-06-05T08:31:10.537885Z",
     "shell.execute_reply.started": "2025-06-05T08:31:10.531820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "def get_prediction(final_parameters):\n",
    "    inference_prompt = format_inference_prompt(final_parameters)\n",
    "    # Tokenize the prompt\n",
    "    device = \"cpu\" # Explicitly set to CPU\n",
    "    inputs = tokenizer(inference_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    print(\"\\nGenerating response (this will be slow on CPU)...\")\n",
    "    with torch.no_grad(): # Ensure no gradients are calculated during inference\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id, # Use the configured pad_token_id\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "    full_response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\n--- Full Generated Text (with prompt) ---\")\n",
    "    print(full_response_text)\n",
    "    response_marker = \"### Response:\\n\"\n",
    "    prompt_length_in_tokens = inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs[0][prompt_length_in_tokens:]\n",
    "    response_only_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    print(\"\\n--- Model's Recommendation ---\")\n",
    "    print(response_only_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:31:13.827798Z",
     "iopub.status.busy": "2025-06-05T08:31:13.826905Z",
     "iopub.status.idle": "2025-06-05T08:31:13.832278Z",
     "shell.execute_reply": "2025-06-05T08:31:13.831322Z",
     "shell.execute_reply.started": "2025-06-05T08:31:13.827746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "parameters ={'Temparature': 30.0,\n",
    " 'Humidity': 45.0,\n",
    " 'Moisture': 12.0,\n",
    " 'Soil Type': 'red',\n",
    " 'Nitrogen': 45,\n",
    " 'Potassium': 3,\n",
    " 'Phosphorous': 34}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:31:33.540469Z",
     "iopub.status.busy": "2025-06-05T08:31:33.539813Z",
     "iopub.status.idle": "2025-06-05T08:32:13.604749Z",
     "shell.execute_reply": "2025-06-05T08:32:13.603899Z",
     "shell.execute_reply.started": "2025-06-05T08:31:33.540441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response (this will be slow on CPU)...\n",
      "\n",
      "--- Full Generated Text (with prompt) ---\n",
      "### Instruction:\n",
      "Given the following soil and environmental parameters:\n",
      "- Temperature: 30.0°C\n",
      "- Humidity: 45.0%\n",
      "- Moisture: 12.0\n",
      "- Soil Type: red\n",
      "- Nitrogen: 45 ppm\n",
      "- Potassium: 3 ppm\n",
      "- Phosphorous: 34 ppm\n",
      "\n",
      "Predict the suitable Crop Type and Fertilizer Name, and provide brief information about how they work or their characteristics.\n",
      "\n",
      "### Response:\n",
      "Recommended Crop Type: Oil seeds\n",
      "Recommended Fertilizer: 10-26-26\n",
      "\n",
      "\n",
      "--- Model's Recommendation ---\n",
      "Recommended Crop Type: Oil seeds\n",
      "Recommended Fertilizer: 10-26-26\n"
     ]
    }
   ],
   "source": [
    "prediction = get_prediction(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
