{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2\n# import unsloth # Remove\nimport torch\n# from unsloth import FastLanguageModel # Remove\nimport pickle # Not used in the provided snippet, can be removed if not used elsewhere\nimport os # Not used in the provided snippet, can be removed if not used elsewhere\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer \nfrom peft import PeftModel # Add\nprint(f\"PyTorch version: {torch.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:27:03.644512Z","iopub.execute_input":"2025-06-05T08:27:03.644878Z","iopub.status.idle":"2025-06-05T08:27:33.371508Z","shell.execute_reply.started":"2025-06-05T08:27:03.644844Z","shell.execute_reply":"2025-06-05T08:27:33.370772Z"}},"outputs":[{"name":"stderr","text":"2025-06-05 08:27:19.507612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749112039.715972      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749112039.782291      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 4\nadapter_repo_id = \"aryan6637/deepseek-crop-fertilizer-info-v3\"\nbase_model_name = \"deepseek-ai/deepseek-llm-7b-base\" # Base model used for fine-tuning\nmax_seq_length = 1024","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:28:27.221423Z","iopub.execute_input":"2025-06-05T08:28:27.221806Z","iopub.status.idle":"2025-06-05T08:28:27.226685Z","shell.execute_reply.started":"2025-06-05T08:28:27.221780Z","shell.execute_reply":"2025-06-05T08:28:27.225390Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Cell 5\nprint(f\"Loading base model {base_model_name} for CPU...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    torch_dtype=torch.float32,  # Use float32 for CPU; bfloat16 might work on some newer CPUs\n    # device_map=\"auto\" will try to use GPU if available, then CPU.\n    # Forcing CPU:\n    # device_map=\"cpu\", # Or handle with .to('cpu') later\n)\n\nprint(f\"Loading tokenizer from {adapter_repo_id}...\")\ntokenizer = AutoTokenizer.from_pretrained(adapter_repo_id, trust_remote_code=True) # Some tokenizers need this\n\nprint(f\"Loading adapter from {adapter_repo_id} and merging...\")\n# Load the LoRA adapter\nmodel = PeftModel.from_pretrained(model, adapter_repo_id)\n# Merge the adapter into the base model for potentially faster CPU inference\n# This increases memory usage as it creates a full model.\nmodel = model.merge_and_unload()\n\nmodel.to(\"cpu\") # Explicitly move to CPU\nprint(\"Model and tokenizer loaded successfully on CPU.\")\nprint(\"WARNING: Running a 7B parameter model on CPU will be very slow for inference.\")\n\n\n# Handle tokenizer padding - Unsloth logs indicate <|PAD_TOKEN|> is used.\nif tokenizer.pad_token is None or tokenizer.pad_token_id is None:\n    print(\"Tokenizer does not have a pad token or pad_token_id. Adding '<|PAD_TOKEN|>'.\")\n    # Check if the token already exists to avoid adding duplicates if config is weird\n    if '<|PAD_TOKEN|>' not in tokenizer.get_vocab():\n        tokenizer.add_special_tokens({'pad_token': '<|PAD_TOKEN|>'})\n    else:\n        tokenizer.pad_token = '<|PAD_TOKEN|>'\n    # After adding or setting, ensure pad_token_id is correct\n    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n\n    # Resize model embeddings if new tokens were truly added (not just setting existing)\n    # This is crucial if add_special_tokens actually modified the vocab size\n    model.resize_token_embeddings(len(tokenizer))\n    print(f\"Resized model embeddings to: {len(tokenizer)}\")\n\n\n# Ensure the model's config pad_token_id is also updated to match the tokenizer\nif model.config.pad_token_id is None or model.config.pad_token_id != tokenizer.pad_token_id:\n    print(f\"Updating model.config.pad_token_id to {tokenizer.pad_token_id}\")\n    model.config.pad_token_id = tokenizer.pad_token_id\n\nprint(f\"Pad token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")\nprint(f\"EOS token: {tokenizer.eos_token}, ID: {tokenizer.eos_token_id}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:28:27.621749Z","iopub.execute_input":"2025-06-05T08:28:27.622070Z","iopub.status.idle":"2025-06-05T08:30:35.540432Z","shell.execute_reply.started":"2025-06-05T08:28:27.622047Z","shell.execute_reply":"2025-06-05T08:30:35.539072Z"}},"outputs":[{"name":"stdout","text":"Loading base model deepseek-ai/deepseek-llm-7b-base for CPU...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/584 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4ff5c2675294c6f8eb700c33e74af53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/22.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0ee8592d2b844d69f85ea351dc6cb7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fcad5cb615741a68a105efb34fa9919"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96906b0faf384f0a80ee4b91138ed498"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.85G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8a58bb309b0484c8e3973c651f0794f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed12826bf4dd4e7f8b7b6d0aec770154"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"697725b0e14a498dae663f4ca3d86219"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d11e142336c4ca1a9d5287277d3643a"}},"metadata":{}},{"name":"stdout","text":"Loading tokenizer from aryan6637/deepseek-crop-fertilizer-info-v3...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.28k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aae2ddf8964445295c3c336f82779be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.50M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecbf26aaaf09462587f6c1b67a4eadc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/355 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a744d855895649f28be2a7924a8bf682"}},"metadata":{}},{"name":"stdout","text":"Loading adapter from aryan6637/deepseek-crop-fertilizer-info-v3 and merging...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cbe9349e8dd4ad5b5b59a67547e6e52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/150M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c4610caae2e40599c984604bde8cf2e"}},"metadata":{}},{"name":"stdout","text":"Model and tokenizer loaded successfully on CPU.\nWARNING: Running a 7B parameter model on CPU will be very slow for inference.\nUpdating model.config.pad_token_id to 100015\nPad token: <|PAD_TOKEN|>, ID: 100015\nEOS token: <｜end▁of▁sentence｜>, ID: 100001\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def format_inference_prompt(final_parameters):\n    instruction_text = (\n        f\"Given the following soil and environmental parameters:\\n\"\n        f\"- Temperature: {final_parameters['Temparature']}°C\\n\"\n        f\"- Humidity: {final_parameters['Humidity']}%\\n\"\n        f\"- Moisture: {final_parameters['Moisture']}\\n\"\n        f\"- Soil Type: {final_parameters['Soil Type']}\\n\"\n        f\"- Nitrogen: {final_parameters['Nitrogen']} ppm\\n\"\n        f\"- Potassium: {final_parameters['Potassium']} ppm\\n\"\n        f\"- Phosphorous: {final_parameters['Phosphorous']} ppm\\n\\n\"\n        f\"Predict the suitable Crop Type and Fertilizer Name, and provide brief information about how they work or their characteristics.\"\n    )\n    # Alpaca format - the model was trained to generate text after \"### Response:\\n\"\n    formatted_prompt = f\"### Instruction:\\n{instruction_text}\\n\\n### Response:\\n\"\n    return formatted_prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:31:09.769638Z","iopub.execute_input":"2025-06-05T08:31:09.769973Z","iopub.status.idle":"2025-06-05T08:31:09.775233Z","shell.execute_reply.started":"2025-06-05T08:31:09.769951Z","shell.execute_reply":"2025-06-05T08:31:09.774311Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Cell 8\ndef get_prediction(final_parameters):\n    inference_prompt = format_inference_prompt(final_parameters)\n    # Tokenize the prompt\n    device = \"cpu\" # Explicitly set to CPU\n    inputs = tokenizer(inference_prompt, return_tensors=\"pt\").to(device)\n\n    print(\"\\nGenerating response (this will be slow on CPU)...\")\n    with torch.no_grad(): # Ensure no gradients are calculated during inference\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id, # Use the configured pad_token_id\n            do_sample=True,\n            temperature=0.6,\n            top_p=0.9,\n        )\n\n    full_response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(\"\\n--- Full Generated Text (with prompt) ---\")\n    print(full_response_text)\n    response_marker = \"### Response:\\n\"\n    prompt_length_in_tokens = inputs.input_ids.shape[1]\n    generated_tokens = outputs[0][prompt_length_in_tokens:]\n    response_only_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n    print(\"\\n--- Model's Recommendation ---\")\n    print(response_only_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:31:10.531012Z","iopub.execute_input":"2025-06-05T08:31:10.531848Z","iopub.status.idle":"2025-06-05T08:31:10.538853Z","shell.execute_reply.started":"2025-06-05T08:31:10.531820Z","shell.execute_reply":"2025-06-05T08:31:10.537885Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"parameters ={'Temparature': 30.0,\n 'Humidity': 45.0,\n 'Moisture': 12.0,\n 'Soil Type': 'red',\n 'Nitrogen': 45,\n 'Potassium': 3,\n 'Phosphorous': 34}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:31:13.826905Z","iopub.execute_input":"2025-06-05T08:31:13.827798Z","iopub.status.idle":"2025-06-05T08:31:13.832278Z","shell.execute_reply.started":"2025-06-05T08:31:13.827746Z","shell.execute_reply":"2025-06-05T08:31:13.831322Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"prediction = get_prediction(parameters)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:31:33.539813Z","iopub.execute_input":"2025-06-05T08:31:33.540469Z","iopub.status.idle":"2025-06-05T08:32:13.604749Z","shell.execute_reply.started":"2025-06-05T08:31:33.540441Z","shell.execute_reply":"2025-06-05T08:32:13.603899Z"}},"outputs":[{"name":"stdout","text":"\nGenerating response (this will be slow on CPU)...\n\n--- Full Generated Text (with prompt) ---\n### Instruction:\nGiven the following soil and environmental parameters:\n- Temperature: 30.0°C\n- Humidity: 45.0%\n- Moisture: 12.0\n- Soil Type: red\n- Nitrogen: 45 ppm\n- Potassium: 3 ppm\n- Phosphorous: 34 ppm\n\nPredict the suitable Crop Type and Fertilizer Name, and provide brief information about how they work or their characteristics.\n\n### Response:\nRecommended Crop Type: Oil seeds\nRecommended Fertilizer: 10-26-26\n\n\n--- Model's Recommendation ---\nRecommended Crop Type: Oil seeds\nRecommended Fertilizer: 10-26-26\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"prediction","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}